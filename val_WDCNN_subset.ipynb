{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidan-sc/val_WDCNN_subset/blob/main/val_WDCNN_subset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82b5be2e-bf06-4fca-a12a-ee6d11f6236c"
      },
      "source": [
        "## Data preprocessing"
      ],
      "id": "82b5be2e-bf06-4fca-a12a-ee6d11f6236c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Ho4HGvROgD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "print(f\"cwd:{os.getcwd()}\")\n",
        "\n",
        "if os.getcwd() != 'content/drive/MyDrive/paderborn':\n",
        "  os.chdir('./drive/MyDrive/paderborn')"
      ],
      "id": "g0Ho4HGvROgD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU4H271jSS5E"
      },
      "outputs": [],
      "source": [],
      "id": "rU4H271jSS5E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1068b362-f7d9-4ad4-a029-bfac21d2e283"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from paderborn_data_loader_subset import PaderbornData"
      ],
      "id": "1068b362-f7d9-4ad4-a029-bfac21d2e283"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "OGY0_1foSefc"
      },
      "id": "OGY0_1foSefc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0baed5e2-8584-4abb-b268-7332028b156c"
      },
      "outputs": [],
      "source": [
        "faults_train = {\n",
        "    # normal healthy bearings\n",
        "    'K001': 0,\n",
        "    #'K002': 0,\n",
        "    #'K003': 0,\n",
        "    # 'K004': 0,\n",
        "    # 'K005': 0,\n",
        "    # 'K006': 0,\n",
        "    # artificial damage\n",
        "    'KA01': 1,\n",
        "    #'KA03': 1,\n",
        "    'KA05': 1,\n",
        "    # 'KA06': 1,\n",
        "    'KA07': 1,\n",
        "    #'KA08': 1,\n",
        "    #'KA09': 1,\n",
        "    'KI01': 2,\n",
        "    #'KI03': 2,\n",
        "    'KI05': 2,\n",
        "    'KI07': 2,\n",
        "    #'KI08': 2,\n",
        "    # real damage\n",
        "    #'KI04': 1,\n",
        "    #'KI14': 1,\n",
        "    # 'KI16': 1,\n",
        "    # 'KI17': 1,\n",
        "    # 'KI18': 1,\n",
        "    # 'KI21': 1,\n",
        "    # 'KA04': 2,\n",
        "    # 'KA15': 2,\n",
        "    #'KA16': 2,\n",
        "    #'KA22': 2,\n",
        "    # 'KA30': 2,\n",
        "    #'KB23': 'IROR',\n",
        "    #'KB24': 'IROR',\n",
        "    #'KB27': 'IROR',\n",
        "}\n",
        "\n",
        "faults_test = {\n",
        "    # normal healthy bearings\n",
        "    #'K001': 0,\n",
        "    'K002': 0,\n",
        "    # 'K003': 0,\n",
        "    #'K004': 0,\n",
        "    # 'K005': 0,\n",
        "    # 'K006': 0,\n",
        "    # artificial damage\n",
        "    #'KA01': 1,\n",
        "    #'KA03': 1,\n",
        "    #'KA05': 1,\n",
        "    # 'KA06': 1,\n",
        "    #'KA07': 1,\n",
        "    # 'KA08': 1,\n",
        "    # 'KA09': 1,\n",
        "    #'KI01': 2,\n",
        "    # 'KI03': 2,\n",
        "    #'KI05': 2,\n",
        "    #'KI07': 2,\n",
        "    # 'KI08': 2,\n",
        "    # real damage\n",
        "    #'KI04': 1,\n",
        "    'KI14': 1,\n",
        "    'KI16': 1,\n",
        "    'KI17': 1,\n",
        "    'KI18': 1,\n",
        "    'KI21': 1,\n",
        "    'KA04': 2,\n",
        "    'KA15': 2,\n",
        "    'KA16': 2,\n",
        "    #'KA22': 2,\n",
        "    'KA30': 2,\n",
        "    #'KB23': 'IROR',\n",
        "    #'KB24': 'IROR',\n",
        "    #'KB27': 'IROR',\n",
        "}"
      ],
      "id": "0baed5e2-8584-4abb-b268-7332028b156c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cf618b6-d37b-4d4c-9efa-78f3428a603a"
      },
      "source": [
        "### Split the Paderborn bearing data into training and testing sets"
      ],
      "id": "1cf618b6-d37b-4d4c-9efa-78f3428a603a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7842d151-ee4b-44f1-9742-480665bf11c4"
      },
      "outputs": [],
      "source": [
        "root_dir = './data/raw/'\n",
        "experiment = PaderbornData(root_dir, experiment='artificial', datastream='vibration', normalisation='robust-zscore')\n",
        "\n",
        "x_data, y_data, _, _ = experiment.split_data(250000,\n",
        "                                                         train_fraction=1,\n",
        "                                                         window_step=1024,\n",
        "                                                         window_length=4500,\n",
        "                                                         faults_idx=faults_train,\n",
        "                                                         verbose=False)"
      ],
      "id": "7842d151-ee4b-44f1-9742-480665bf11c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09d387f6-3e7a-45a9-aab0-c26ce09fcb32"
      },
      "outputs": [],
      "source": [
        "print(x_data.shape)\n",
        "print(y_data.shape)"
      ],
      "id": "09d387f6-3e7a-45a9-aab0-c26ce09fcb32"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694b77ea-7693-49ab-ad99-8dc40539a46a"
      },
      "source": [
        "## Train our WDCNN model\n",
        "\n",
        "### First split the data, scale it, and convert labels to one hot encoding"
      ],
      "id": "694b77ea-7693-49ab-ad99-8dc40539a46a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7786d99c-d506-43ea-80d0-b68f7da7af30"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ],
      "id": "7786d99c-d506-43ea-80d0-b68f7da7af30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fd06706-50cf-4afb-9954-46c464e5c14c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# y_train = to_categorical(y_train)\n",
        "# y_test = to_categorical(y_test)"
      ],
      "id": "6fd06706-50cf-4afb-9954-46c464e5c14c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8159a97e-a57e-417a-9b88-545752cde565"
      },
      "outputs": [],
      "source": [],
      "id": "8159a97e-a57e-417a-9b88-545752cde565"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80104235-d9ed-4e5d-89e3-cddaaaa50056"
      },
      "source": [
        "### Build our WDCNN model"
      ],
      "id": "80104235-d9ed-4e5d-89e3-cddaaaa50056"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4ebed56-3feb-4a1e-9640-7e78eab68be3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# build the wdcnn model\n",
        "def generate_model(n_class, n_timesteps, n_variables, first_kernel=64):\n",
        "\n",
        "    # set up the shape of the input\n",
        "    ip = Input(shape=(n_timesteps, n_variables))\n",
        "\n",
        "    # convolutional layers\n",
        "    y = Conv1D(16, (first_kernel), strides=16, padding='same')(ip)\n",
        "    y = Activation('relu')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling1D(2, strides=2, padding='same')(y)\n",
        "\n",
        "    y = Conv1D(32, (3), padding='same')(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling1D(2, strides=2, padding='same')(y)\n",
        "\n",
        "    y = Conv1D(64, (3), padding='same')(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling1D(2, strides=2, padding='same')(y)\n",
        "\n",
        "    y = Conv1D(64, (3), padding='same')(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling1D(2, strides=2, padding='same')(y)\n",
        "\n",
        "    y = Conv1D(64, (3), padding='same')(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = MaxPooling1D(2, strides=2, padding='same')(y)\n",
        "\n",
        "    # flatten\n",
        "    y = Flatten()(y)\n",
        "\n",
        "    # dense\n",
        "    y = Dense(100)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # add the softmax classification outpuy\n",
        "    out = Dense(n_class, activation='softmax')(y)\n",
        "\n",
        "    # join the input and the output and return the model\n",
        "    model = Model(ip, out)\n",
        "\n",
        "    return model\n"
      ],
      "id": "c4ebed56-3feb-4a1e-9640-7e78eab68be3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ca736a-d4df-4afa-be7b-efd679c44bb7"
      },
      "outputs": [],
      "source": [
        "# model = generate_model(3, x_train.shape[1], x_train.shape[2], first_kernel=256)"
      ],
      "id": "79ca736a-d4df-4afa-be7b-efd679c44bb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e967fd0f-1e35-475f-95c2-1de7b0961bf0"
      },
      "outputs": [],
      "source": [],
      "id": "e967fd0f-1e35-475f-95c2-1de7b0961bf0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11da108e-00a4-4dae-bd8a-9e4cb50b3b64"
      },
      "source": [
        "### Train the model"
      ],
      "id": "11da108e-00a4-4dae-bd8a-9e4cb50b3b64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f60b9cb-cd81-4569-8d23-1d9cda139054"
      },
      "outputs": [],
      "source": [
        "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics='acc')"
      ],
      "id": "7f60b9cb-cd81-4569-8d23-1d9cda139054"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45baea20-e7dd-447a-bc81-09a1bebe82b7"
      },
      "outputs": [],
      "source": [
        "# val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
        "# history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=val_dataset)"
      ],
      "id": "45baea20-e7dd-447a-bc81-09a1bebe82b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihOCpCex4_8Q"
      },
      "source": [
        "# Validate the Model"
      ],
      "id": "ihOCpCex4_8Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUqron2S48yF"
      },
      "outputs": [],
      "source": [
        "# set the root directory for results\n",
        "results_dir = ('./weights/wdcnn_loads/' +\n",
        "               'cross_validation/{0}').format(time.strftime(\"%Y%m%d_%H%M\"))"
      ],
      "id": "YUqron2S48yF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d349a23-1334-470d-8e04-bbe1b1719193"
      },
      "outputs": [],
      "source": [
        "from pickle import FALSE\n",
        "\n",
        "import gc\n",
        "# progress ...\n",
        "print('doing cross validation ...')\n",
        "\n",
        "# fix random seed for reproduciblity\n",
        "seed = 1337\n",
        "np.random.seed(seed)\n",
        "\n",
        "# store the accuracy results\n",
        "accuracies = [list() for i in range(len(x_data))]\n",
        "\n",
        "# store the ground truths and predictions for target load(s) (for each fold)\n",
        "# so we can produce confusion plots later\n",
        "ground_truth = [list() for i in range(len(x_data))]\n",
        "predictions  = [list() for i in range(len(x_data))]\n",
        "\n",
        "xval_history = list()\n",
        "final_accuracy = list()\n",
        "final_loss = list()\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend\n",
        "from numba import cuda\n",
        "\n",
        "# create the kfold object with 10 splits\n",
        "n_folds = 10\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "# run the crossvalidation\n",
        "fold = 0\n",
        "i=False\n",
        "\n",
        "for train_index, test_index in kf.split(x_data, y_data):\n",
        "    if i == True:\n",
        "      del model, t_data, x_data_t, y_data_t, x_train, y_train, x_test, y_test\n",
        "      gc.collect()\n",
        "      tf.keras.backend.clear_session()\n",
        "    # progress ...\n",
        "    print('evaluating fold {0}'.format(fold))\n",
        "\n",
        "    # set up a model checkpoint callback (including making the directory where\n",
        "    # to save our weights)\n",
        "    directory = results_dir + 'fold_{0}/'.format(fold)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    filename  = 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
        "    checkpointer = ModelCheckpoint(filepath=directory+filename,\n",
        "                                   verbose=0,\n",
        "                                   save_best_only=True)\n",
        "    # get train and test data\n",
        "    x_train, y_train = x_data[train_index], y_data[train_index]\n",
        "    x_test, y_test   = x_data[test_index], y_data[test_index]\n",
        "\n",
        "    # one hot encode the labels\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test  = to_categorical(y_test)\n",
        "\n",
        "    # build and compile the model\n",
        "    model = generate_model(3, x_train.shape[1], x_train.shape[2], first_kernel=256)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics='acc')\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
        "    history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=val_dataset)\n",
        "\n",
        "    # save the final model\n",
        "    model.save(directory + 'final_model.h5')\n",
        "\n",
        "    # store the training history\n",
        "    xval_history.append(history.history)\n",
        "\n",
        "    # print the validation result\n",
        "    final_loss.append(history.history['val_loss'][-1])\n",
        "    final_accuracy.append(history.history['val_acc'][-1])\n",
        "    print('validation loss is {0} and accuracy is {1}'.format(final_loss[-1],\n",
        "          final_accuracy[-1]))\n",
        "\n",
        "    # get the data (using testing sets - i.e. non overlapping windows)\n",
        "    #t_data = PaderbornData(data_path, experiment, [l], normalisation=normalisation)\n",
        "    t_data = PaderbornData(root_dir, experiment='real', datastream='vibration', normalisation='robust-zscore')\n",
        "\n",
        "    # _, _, x_data_t, y_data_t = t_data.split_data(360000,\n",
        "    #                                                     train_fraction=0.0,\n",
        "    #                                                     window_length=window_length,\n",
        "    #                                                     verbose=False)\n",
        "\n",
        "    experiment = PaderbornData(root_dir, experiment='real', datastream='vibration', normalisation='robust-zscore')\n",
        "    _, _, x_data_t, y_data_t = experiment.split_data(250000,\n",
        "                                                            train_fraction=0,\n",
        "                                                            window_step=2048,\n",
        "                                                            window_length=4500,\n",
        "                                                            faults_idx=faults_test,\n",
        "                                                            verbose=False)\n",
        "\n",
        "    # reformat the data for use in predictions\n",
        "    x_t = x_data_t\n",
        "    y_true_t = y_data_t\n",
        "    y_t = to_categorical(y_data_t)\n",
        "\n",
        "    # evaluate\n",
        "    scores_t = model.evaluate(x=x_t, y=y_t, batch_size=128, verbose=0)\n",
        "    print('scores = {0}'.format(scores_t[1]))\n",
        "\n",
        "    # what do we get wrong?\n",
        "    y_pred_t = model.predict(x_t)\n",
        "\n",
        "    # # store the results\n",
        "    # accuracies[ix].append(scores_t[1])\n",
        "    # ground_truth[ix].append(y_t)\n",
        "    # predictions[ix].append(y_pred_t)\n",
        "\n",
        "\n",
        "    # display the results for this fold so we can see how we are doing ...\n",
        "\n",
        "    # done testing generalisation ...\n",
        "\n",
        "\n",
        "    # next fold ...\n",
        "    fold = fold + 1"
      ],
      "id": "1d349a23-1334-470d-8e04-bbe1b1719193"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43346de0-ff94-465e-a591-75e1325c67ab"
      },
      "source": [
        "### Plot some results"
      ],
      "id": "43346de0-ff94-465e-a591-75e1325c67ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa1d703b-8eb1-4abc-aee8-b12b4217895e"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "fa1d703b-8eb1-4abc-aee8-b12b4217895e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5888c08b-9ae7-411b-b734-855408fe48fd"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "5888c08b-9ae7-411b-b734-855408fe48fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cccbd980-4154-44a4-846c-9fa5f8384ea2"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(val_dataset)\n",
        "print(\"loss: %.2f\" % loss)\n",
        "print(\"acc:  %.2f\" % acc)"
      ],
      "id": "cccbd980-4154-44a4-846c-9fa5f8384ea2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30f589f9-6296-4fce-989d-9b354bafc70e"
      },
      "outputs": [],
      "source": [],
      "id": "30f589f9-6296-4fce-989d-9b354bafc70e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d01dfc5e-a3e4-464b-8214-74ea5c433faa"
      },
      "source": [
        "### Get the confusion matrix to see what we struggle with"
      ],
      "id": "d01dfc5e-a3e4-464b-8214-74ea5c433faa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea3997e6-2f1c-436d-b7f9-b3aa1302d314"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "visualisation_utils.py\n",
        "\n",
        "make pretty graphs to show classifier performance\n",
        "\n",
        "(most of these are based on the really useful examples from the\n",
        "scikit learn user guides!)\n",
        "\n",
        "author:     alex shenfield\n",
        "date:       27/04/2018\n",
        "\"\"\"\n",
        "\n",
        "# numpy is needed for everything :)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# utilities for managing the data\n",
        "import itertools\n",
        "\n",
        "# data analysis functions from scikit learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# get the classes and actually plot the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    classes = np.unique(y_true)\n",
        "    plot_cm(cm, classes=classes, title=None)\n",
        "\n",
        "\n",
        "# define a function for plotting a confusion matrix\n",
        "def plot_cm(cm,\n",
        "            classes,\n",
        "            normalize=False,\n",
        "            title='Confusion matrix',\n",
        "            cmap=plt.cm.Blues):\n",
        "\n",
        "    # should we normalise the confusion matrix?\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print('Confusion matrix, with normalization')\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # display in command windows\n",
        "    print(cm)\n",
        "\n",
        "    # create a plot for the confusion matrix\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # if we want a title displayed\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "    fmt = '.3f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "id": "ea3997e6-2f1c-436d-b7f9-b3aa1302d314"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15b10c41-92b1-4e3b-8169-3b53c51a5d44"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(val_dataset)\n",
        "print(predictions.shape)"
      ],
      "id": "15b10c41-92b1-4e3b-8169-3b53c51a5d44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fe5f2d9-b080-427f-b08d-8371584f3e21"
      },
      "outputs": [],
      "source": [
        "y_true = np.argmax(y_test, axis=1)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "print(y_true.shape)\n",
        "print(y_pred.shape)"
      ],
      "id": "5fe5f2d9-b080-427f-b08d-8371584f3e21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e877e6db-94c7-4d38-b361-ea87f752fb02"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_true, y_pred)"
      ],
      "id": "e877e6db-94c7-4d38-b361-ea87f752fb02"
    },
    {
      "cell_type": "code",
      "source": [
        "end_time = time.time()\n",
        "\n",
        "print(end_time - start_time)"
      ],
      "metadata": {
        "id": "aFF8l0g6SjFE"
      },
      "id": "aFF8l0g6SjFE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}